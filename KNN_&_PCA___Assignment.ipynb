{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "Answer:\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression. It works on the idea that similar data points are close to each other.\n",
        "\n",
        "In KNN, when a new data point is given, the algorithm:\n",
        "\n",
        "Calculates the distance between the new point and all training data points.\n",
        "\n",
        "Selects the K closest neighbors.\n",
        "\n",
        "Makes a prediction based on these neighbors.\n",
        "\n",
        "In classification, the class with the majority vote among the K neighbors is assigned.\n",
        "\n",
        "In regression, the average value of the K neighbors is used as the prediction.\n",
        "\n",
        "KNN is simple, intuitive, and works well when the dataset is not very large."
      ],
      "metadata": {
        "id": "u6NrC2S71NfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "Answer:\n",
        "\n",
        "The Curse of Dimensionality refers to problems that arise when the number of features (dimensions) becomes very large.\n",
        "\n",
        "In high-dimensional data:\n",
        "\n",
        "Distance between data points becomes less meaningful.\n",
        "\n",
        "All points appear almost equally distant.\n",
        "\n",
        "KNN struggles to find truly “nearest” neighbors.\n",
        "\n",
        "This negatively affects KNN because it relies heavily on distance calculations. As dimensions increase, model accuracy decreases and computation becomes expensive.\n",
        "\n",
        "Dimensionality reduction techniques like PCA are often used to overcome this problem"
      ],
      "metadata": {
        "id": "l_uSazR51NcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "Answer:\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms original features into a smaller set of new features called principal components.\n",
        "\n",
        "These components:\n",
        "\n",
        "Are uncorrelated\n",
        "\n",
        "Capture maximum variance in the data\n",
        "\n",
        "Difference between PCA and Feature Selection:\n",
        "\n",
        "PCA\tFeature Selection\n",
        "Creates new features\tSelects existing features\n",
        "Uses transformation\tNo transformation\n",
        "Reduces multicollinearity\tKeeps original meaning\n",
        "\n",
        "PCA is mainly used when features are highly correlated."
      ],
      "metadata": {
        "id": "ylJz5Ifq1NZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "Answer:\n",
        "\n",
        "In PCA:\n",
        "\n",
        "Eigenvectors represent the directions of maximum variance.\n",
        "\n",
        "Eigenvalues represent the amount of variance captured in those directions.\n",
        "\n",
        "Eigenvectors define the principal components, while eigenvalues tell us how important each component is.\n",
        "\n",
        "Higher eigenvalue → more information captured\n",
        "Lower eigenvalue → less useful component\n",
        "\n",
        "They help decide which components to keep and which to discard."
      ],
      "metadata": {
        "id": "DT4TWpCv1NWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "Answer:\n",
        "\n",
        "PCA reduces the number of features and removes noise, while KNN performs classification based on distance.\n",
        "\n",
        "When combined:\n",
        "\n",
        "PCA reduces dimensionality → distances become meaningful\n",
        "\n",
        "KNN becomes faster and more accurate\n",
        "\n",
        "Overfitting is reduced\n",
        "\n",
        "This makes the PCA + KNN pipeline efficient for high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "U1CrqFI01NUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling"
      ],
      "metadata": {
        "id": "qj2Rs2FQ1NRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# KNN without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "acc_without_scaling = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with scaling\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_with_scaling = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_without_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqoXKtgc2m0i",
        "outputId": "cbde450e-c7f2-4a72-fcec-b893aff2dd1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Train a PCA model and print explained variance ratio"
      ],
      "metadata": {
        "id": "xYSRlizz1NEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mr_xri2zWzt",
        "outputId": "bf5a6801-064d-45bb-8e96-52fc009ca2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Scale data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Train KNN on PCA-transformed data (top 2 components)"
      ],
      "metadata": {
        "id": "5pZqKlrk20hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Scaling\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA (2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "print(\"Accuracy with PCA (2 components):\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fmIUxHV25iQ",
        "outputId": "06bdc45d-0c4e-44fd-d842-b45117f4cc42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: KNN with different distance metrics"
      ],
      "metadata": {
        "id": "o-7_MDJE20eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Scale\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Euclidean\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n",
        "\n",
        "# Manhattan\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FplXeLSC3CaN",
        "outputId": "2cb32cfb-67d5-4049-8f11-c249c4e64128"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Accuracy: 0.9629629629629629\n",
            "Manhattan Accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: High-dimensional Gene Expression Dataset (Conceptual)\n",
        "Answer:\n",
        "\n",
        "PCA for Dimensionality Reduction\n",
        "PCA is used to reduce thousands of gene features into fewer principal components while preserving important variance.\n",
        "\n",
        "Choosing Number of Components\n",
        "Use explained variance ratio and select components that retain 90–95% variance.\n",
        "\n",
        "KNN after PCA\n",
        "Apply KNN on reduced features to improve accuracy and reduce overfitting.\n",
        "\n",
        "Evaluation\n",
        "Use cross-validation, accuracy, precision-recall, and ROC-AUC.\n",
        "\n",
        "Business Justification\n",
        "\n",
        "Reduces noise and overfitting\n",
        "\n",
        "Improves prediction reliability\n",
        "\n",
        "Makes model interpretable and computationally efficient\n",
        "\n",
        "This pipeline provides a robust and realistic solution for biomedical data analysis."
      ],
      "metadata": {
        "id": "dfQClZeF20bm"
      }
    }
  ]
}